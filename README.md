
# Predicting the likelihood of a personal name being classified as male or female.

## Installation

```
git clone https://github.com/junwang4/name-to-gender-inference
cd name-to-gender-inference
pip install -r requirements.txt
```

## Usage

### STEP 1. Prepare you data (for example, see file `./data/sample.csv`)
Format: `family name, first and/or middle name`
```
name
"Nawab, Amber"
"Rittinghaus, Klaus"
"Ben Moshe, Shlomit"
"Agrawal, Damyanti"
"Adkin, Allan L"
"Park, Kaechang"
```

### STEP 2. Apply the inference model (located in: `./models`) to your data of personal names
```
python gender_inference.py --task=predict --input=data/sample.csv
```

The gender prediction result will be saved to `data/sample.pred.csv`, with the following format:
```
name,male_confidence
"Nawab, Amber",0.01
"Rittinghaus, Klaus",1.00
"Ben Moshe, Shlomit",0.03
"Agrawal, Damyanti",0.16
"Adkin, Allan L",0.99
"Park, Kaechang",0.96
```

## Performance of our model

Santamaría and Mihaljević published a paper:
[Comparison and benchmark of name-to-gender inference services (2018, PeerJ)](https://peerj.com/articles/cs-156/).
They compiled a benchmark dataset of 7076 names which in turn
includes 5779 names with known gender.
They used the 5779 gender-known names 
to examine the performance of five name-to-gender services:
*Gender API*, *gender-guesser*, *genderize.io*, *NameAPI*, and *NamSor*. 

They found that typically **Gender API** achieves the best results.
For example, in Table 9 of their paper (Benchmark 3a: Minimization of misclassifications constrained to a 25% non-classification rate on the entire data set),
they reported: 
> Gender API outperforms the other services with a 0.9% misclassification rate, followed by NamSor with 1.4% and genderize.io with 1.7%.
 
Specifically, the following table shows the performance of
*Gender API* in terms of confusion matrix.


         F pred  M pred  U pred  

      F    1750     172      46              1968
      M     110    3573     128              3811

         precision    recall  f1-score    support

      F      0.941     0.911     0.925       1922
      M      0.954     0.970     0.962       3683

In the table, the column `U pred` shows that
*Gender API* is not sure about the gender of 128 male and 46 female names (in total, 174 unknowns).

```
python gender_inference.py --task=show_gender_api_com_performance
```

To compare with *Gender API*, 
we set the confidence thresholds for identifying Male and Female as 0.602 and 0.543, respectively.
This adjustment ensures that the 'U pred' numbers for both Male and Female 
align (though a bit different) with those generated by the *Gender API*.
(The observation that the female threshold is lower than male is due to the fact that 75% of the
training data consists of male names.)

```
python gender_inference.py --task=advanced_evaluation --input=data/PeerJ_7000.csv --confidence_male_threshold=0.602 --confidence_female_threshold=0.543
```

          F pred  M pred  U pred      

      F    1777     147      44              1968
      M     126    3555     130              3811


         precision    recall  f1-score    support

      F      0.934     0.924     0.929       1924
      M      0.960     0.966     0.963       3681



### Summary of the comparison
The F1-score comparison between our algorithm and Gender API is as follows:

|  | Gender API | Our algorithm |
|--|------------|---------------|
|F | 0.925      | **0.929**     |
|M | 0.962      | 0.963         |


### Conclusion 

- Our model achieves performance comparable to the best among the five name-to-gender inference services.
- Furthermore, our model is open-source and offers probabilistic gender predictions.


## How to find your male/female confidence thresholds

- Set a maximum rejection rate, which defines the proportion of names classified as gender-unknown. A higher rejection rate generally leads to improved precision and recall.
- Specify the search step size (e.g., 0.01 or 0.001). Using a smaller step like 0.001 instead of 0.01 will make the process approximately 100 times slower, as the search spans both male and female dimensions.
- Define the confidence threshold search range. If using a small step size (e.g., 0.001), keep the range as narrow as possible to reduce computation time.

### Case 1.
`max-rejection-rate = 0.125`, `step-size = 0.01`, `threshold-range in [0.60, 0.90]`

```
python find_optimal_threshold.py --max-rejection-rate=0.125 --threshold-range-left=0.60 --threshold-range-right=0.90 --step-size=0.01
```

Output:
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.820
Female Threshold: 0.780

Male Rejection Rate: 0.1241
Female Rejection Rate: 0.1235

Male F1 Score: 0.9835
Female F1 Score: 0.9675

Male Recall: 0.9880
Female Recall: 0.9588

Male Precision: 0.9789
Female Precision: 0.9764
```


Check the performance of the found optimal thresholds
```
python gender_inference.py --task=advanced_evaluation --input=data/PeerJ_7000.csv --confidence_male_threshold=0.82 --confidence_female_threshold=0.78
```
Output

			   F pred  M pred  U pred      

			F    1654      71     243  1968
			M      40    3298     473  3811


              precision    recall  f1-score   support

           F      0.976     0.959     0.968      1725
           M      0.979     0.988     0.983      3338


### Case 2.
`max-rejection-rate = 0.25`, `step-size = 0.002`, `threshold-range in [0.90, 0.95]`

```
python find_optimal_threshold.py --max-rejection-rate=0.25 --threshold-range-left=0.90 --threshold-range-right=0.95 --step-size=0.002
```

Output
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.936
Female Threshold: 0.929

Male Rejection Rate: 0.2393
Female Rejection Rate: 0.2287
Male F1 Score: 0.9935
Female F1 Score: 0.9874
Male Recall: 0.9959
Female Recall: 0.9829
Male Precision: 0.9911
Female Precision: 0.9920

```

Check the performance of the found optimal thresholds
```
python gender_inference.py --task=advanced_evaluation --input=data/PeerJ_7000.csv --confidence_male_threshold=0.936 --confidence_female_threshold=0.929
```
Output

			   F pred  M pred  U pred      

			F    1492      26     450  1968
			M      12    2887     912  3811


              precision    recall  f1-score   support

           F      0.992     0.983     0.987      1518
           M      0.991     0.996     0.993      2899



### Case 3.
`max-rejection-rate = 0.035`, `step-size = 0.001`, `threshold-range in [0.52, 0.62]`

```
python find_optimal_threshold.py --max-rejection-rate=0.035 --threshold-range-left=0.52 --threshold-range-right=0.62 --step-size=0.001
```

Output
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.603
Female Threshold: 0.543

Male Rejection Rate: 0.0344
Female Rejection Rate: 0.0234
Male F1 Score: 0.9633
Female F1 Score: 0.9292
Male Recall: 0.9658
Female Recall: 0.9246
Male Precision: 0.9608
Female Precision: 0.9338
```

Check the performance of the found optimal thresholds
```
python gender_inference.py --task=advanced_evaluation --input=data/PeerJ_7000.csv --confidence_male_threshold=0.603 --confidence_female_threshold=0.543
```
Output

			   F pred  M pred  U pred      

			F    1777     145      46  1968
			M     126    3554     131  3811


              precision    recall  f1-score   support

           F      0.934     0.925     0.929      1922
           M      0.961     0.966     0.963      3680
