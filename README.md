
### Predicting the likelihood of a personal name being classified as male or female.

## Installation

```
git clone https://github.com/junwang4/name-to-gender-inference
cd name-to-gender-inference
pip install -r requirements.txt
```

## Usage

### STEP 1. Prepare you data (for example, see file `./data/sample.csv`)
Format: `family name, first and/or middle name`
```
name
"Nawab, Amber"
"Rittinghaus, Klaus"
"Ben Moshe, Shlomit"
"Agrawal, Damyanti"
"Adkin, Allan L"
"Park, Kaechang"
```

### STEP 2. Apply the inference model (located in: `./models`) to your data of personal names
```
python gender_inference.py --task=predict --input=data/sample.csv
```

The gender prediction result will be saved to `data/sample.pred.csv`, with the following format:
```
name,male_confidence
"Nawab, Amber",0.01
"Rittinghaus, Klaus",1.00
"Ben Moshe, Shlomit",0.03
"Agrawal, Damyanti",0.16
"Adkin, Allan L",0.99
"Park, Kaechang",0.96
```

## Performance of our model

Santamaría and Mihaljević published a paper:
[Comparison and benchmark of name-to-gender inference services (2018, PeerJ)](https://peerj.com/articles/cs-156/).
They compiled a benchmark dataset of 7076 names which in turn
includes 5779 names with known gender.
They used the 5779 gender-known names 
to examine the performance of five name-to-gender services:
*Gender API*, *gender-guesser*, *genderize.io*, *NameAPI*, and *NamSor*. 

They found that typically **Gender API** achieves the best results.
For example, in Table 9 of their paper (Benchmark 3a: Minimization of misclassifications constrained to a 25% non-classification rate on the entire data set),
they reported: 
> Gender API outperforms the other services with a 0.9% misclassification rate, followed by NamSor with 1.4% and genderize.io with 1.7%.
 
Specifically, the following table shows the performance of
*Gender API* in terms of confusion matrix.

|      | M pred | F pred | U pred |
|------|--------|--------|--------|
|M     | 3,573  | 110    | 128    |
|F     | 172    | 1,750  | 46     |

In the table, the column `U pred` shows that
*Gender API* is not sure about the gender of 128 male and 46 female names (in total, 174 unknowns).

To compare with *Gender API*, 
we set the confidence thresholds for identifying Male and Female as 0.602 and 0.543, respectively.
This adjustment ensures that the 'U pred' numbers for both Male and Female 
align (though a bit different) with those generated by the *Gender API*.
(The observation that the female threshold is lower than male is due to the fact that 75% of the
training data consists of male names.)

```
python gender_inference.py --task=advanced_error_analysis --input=data/PeerJ_7000.csv --confidence_male_threshold=0.602 --confidence_female_threshold=0.543
```

|      | M pred | F pred | U pred |
|------|--------|--------|--------|
|M     | 3,555  | 126    | 130    |
|F     | 147    | 1,777  | 44     |


The F1-score comparison between our algorithm and Gender API is as follows:

|  | Gender API | Our algorithm |
|--|------------|---------------|
|M | 0.946      | 0.946         |
|F | 0.914      | **0.918**     |


## Conclusion 

- Our model achieves performance comparable to the best among the five name-to-gender inference services.
- Furthermore, our model is open-source and offers probabilistic gender predictions.


## How to find your male/female confidence thresholds

First, specify a maximum rejection rate, representing the proportion of names to be classified as gender-unknown.
Second, specify the confidence threshold search range.
Third, specify the search step size.

```
python find_optimal_threshold.py --max-rejection-rate=0.125 --threshold-range-left=0.60 --threshold-range-right=0.90 --step-size=0.01
```

Output:
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.820
Female Threshold: 0.780

Male Rejection Rate: 0.1241
Female Rejection Rate: 0.1235
Male F1 Score: 0.9835
Female F1 Score: 0.9675
Male Recall: 0.9880
Female Recall: 0.9588
Male Precision: 0.9789
Female Precision: 0.9764
```

|      | F pred | M pred | U pred |
|------|--------|--------|--------|
|F     | 1654  | 71    | 243     |
|M     | 40    | 3298  | 473     |

```
   precision    recall  f1-score   support
  
F      0.976     0.959     0.968      1725
M      0.979     0.988     0.983      3338
```


```
python find_optimal_threshold.py --max-rejection-rate=0.25 --threshold-range-left=0.90 --threshold-range-right=0.95 --step-size=0.002
```

Output
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.936
Female Threshold: 0.929

Male Rejection Rate: 0.2393
Female Rejection Rate: 0.2287
Male F1 Score: 0.9935
Female F1 Score: 0.9874
Male Recall: 0.9959
Female Recall: 0.9829
Male Precision: 0.9911
Female Precision: 0.9920

```

|      | F pred | M pred | U pred |
|------|--------|--------|--------|
|F     | 1492  | 26    | 450     |
|M     | 12    | 2887  | 912     |

```
   precision    recall  f1-score   support
  
F      0.992     0.983     0.987      1518
M      0.991     0.996     0.993      2899
```


```
python find_optimal_threshold.py --max-rejection-rate=0.035 --threshold-range-left=0.52 --threshold-range-right=0.62 --step-size=0.001
```

Output
```
OPTIMAL THRESHOLDS FOUND

Male Threshold: 0.603
Female Threshold: 0.543

Male Rejection Rate: 0.0344
Female Rejection Rate: 0.0234
Male F1 Score: 0.9633
Female F1 Score: 0.9292
Male Recall: 0.9658
Female Recall: 0.9246
Male Precision: 0.9608
Female Precision: 0.9338
```


|      | F pred | M pred | U pred |
|------|--------|--------|--------|
|F     | 1777  | 145    | 46     |
|M     | 126    | 3554  | 131     |

```
   precision    recall  f1-score   support
  
F      0.934     0.925     0.929      1922
M      0.961     0.966     0.963      3680
```